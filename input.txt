import http.server
import socketserver
import requests

class ProxyHTTPRequestHandler(http.server.BaseHTTPRequestHandler):
    def handle_request(self):
        # Construct the target URL
        target_url = f"http://{self.headers['Host']}{self.path}"

        # Extract headers from the client request
        headers = {key: value for key, value in self.headers.items()}

        # Extract the body (if present)
        content_length = int(self.headers.get('Content-Length', 0))
        body = self.rfile.read(content_length) if content_length > 0 else None

        # Print the intercepted body
        if body:
            print(f"Request Body for {self.command} {self.path}:\n{body.decode('utf-8', errors='replace')}")

        # Forward the request based on the method
        try:
            response = self.forward_to_target(target_url, headers, body)
            self.send_response_to_client(response)
        except Exception as e:
            self.send_error(500, f"Internal server error: {e}")

    def forward_to_target(self, url, headers, body):
        """
        Forward the intercepted request to the target server.
        """
        method = self.command
        if method == "GET":
            return requests.get(url, headers=headers)
        elif method == "POST":
            return requests.post(url, headers=headers, data=body)
        elif method == "PUT":
            return requests.put(url, headers=headers, data=body)
        elif method == "DELETE":
            return requests.delete(url, headers=headers, data=body)
        elif method == "HEAD":
            return requests.head(url, headers=headers)
        elif method == "OPTIONS":
            return requests.options(url, headers=headers)
        elif method == "PATCH":
            return requests.patch(url, headers=headers, data=body)
        else:
            raise ValueError(f"Unsupported HTTP method: {method}")

    def send_response_to_client(self, response):
        """
        Forward the response back to the client.
        """
        # Send the status code
        self.send_response(response.status_code)

        # Send headers
        for key, value in response.headers.items():
            self.send_header(key, value)
        self.end_headers()

        # Send the body
        self.wfile.write(response.content)

    def do_GET(self):
        self.handle_request()

    def do_POST(self):
        self.handle_request()

    def do_PUT(self):
        self.handle_request()

    def do_DELETE(self):
        self.handle_request()

    def do_HEAD(self):
        self.handle_request()

    def do_OPTIONS(self):
        self.handle_request()

    def do_PATCH(self):
        self.handle_request()

# Start the proxy server
def run_proxy(port=8080):
    with socketserver.ThreadingTCPServer(("", port), ProxyHTTPRequestHandler) as httpd:
        print(f"Serving proxy on port {port}")
        httpd.serve_forever()

if __name__ == "__main__":
    run_proxy()


from airllm import AirLLMLlama2

MAX_LENGTH = 128
# could use hugging face model repo id:
model = AirLLMLlama2("garage-bAInd/Platypus2-70B-instruct")

# or use model's local path...
#model = AirLLMLlama2("/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f")

input_text = [
        'What is the capital of United States?',
    ]

input_tokens = model.tokenizer(input_text,
    return_tensors="pt", 
    return_attention_mask=False, 
    truncation=True, 
    max_length=MAX_LENGTH, 
    padding=True)
           
generation_output = model.generate(
    input_tokens['input_ids'].cuda(), 
    max_new_tokens=20,
    use_cache=True,
    return_dict_in_generate=True)

output = model.tokenizer.decode(generation_output.sequences[0])

print(output)
