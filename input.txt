import os
from typing import Annotated, Literal, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, ToolMessage
from langchain_core.tools import tool
# import agentops
from dotenv import load_dotenv

from arize.otel import register

tracer_provider = register(
    space_id = "U3BhY2U6MjYxODI6cUlwUg==",
    api_key = "ak-",
    project_name = "Suresh_test", # name this to whatever you would like
)

from openinference.instrumentation.langchain import LangChainInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor # Optional, for deeper OpenAI traces
 
# Instrument LangChain (which includes LangGraph)
LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
# Optionally instrument OpenAI for deeper traces if using OpenAI models
OpenAIInstrumentor().instrument(tracer_provider=tracer_provider) 
 
load_dotenv()
 

@tool
def get_weather(location: str) -> str:
    """Get the weather for a given location."""
    weather_data = {
        "New York": "Sunny, 72°F",
        "London": "Cloudy, 60°F",
        "Tokyo": "Rainy, 65°F",
        "Paris": "Partly cloudy, 68°F",
        "Sydney": "Clear, 75°F",
    }
    return weather_data.get(location, f"Weather data not available for {location}")


@tool
def calculate(expression: str) -> str:
    """Evaluate a mathematical expression."""
    try:
        result = eval(expression)
        return f"The result is: {result}"
    except Exception as e:
        return f"Error calculating expression: {str(e)}"


tools = [get_weather, calculate]


class AgentState(TypedDict):
    messages: Annotated[list, add_messages]


model = ChatOpenAI(temperature=0, model="gpt-4o-mini").bind_tools(tools)


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    messages = state["messages"]
    last_message = messages[-1]

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"


def call_model(state: AgentState):
    messages = state["messages"]
    response = model.invoke(messages)
    return {"messages": [response]}


def call_tools(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]

    tool_messages = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]

        for tool_obj in tools:
            if tool_obj.name == tool_name:
                result = tool_obj.invoke(tool_args)
                tool_messages.append(ToolMessage(content=str(result), tool_call_id=tool_call["id"]))
                break

    return {"messages": tool_messages}


workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tools)

workflow.set_entry_point("agent")

workflow.add_conditional_edges("agent", should_continue, {"tools": "tools", "end": END})

workflow.add_edge("tools", "agent")

app = workflow.compile()


def run_example():
    print("=== LangGraph + AgentOps Example ===\n")

    queries = [
        "What's the weather in New York and Tokyo?",
        "Calculate 25 * 4 + 10",
        "What's the weather in Paris? Also calculate 100/5",
    ]

    for query in queries:
        print(f"Query: {query}")
        print("-" * 40)

        messages = [HumanMessage(content=query)]
        result = app.invoke({"messages": messages})

        final_message = result["messages"][-1]
        print(f"Response: {final_message.content}\n")


if __name__ == "__main__":
    run_example()
    print("✅ Check your AgentOps dashboard for the trace!")


# # Let's check programmatically that spans were recorded in AgentOps
# print("\n" + "=" * 50)
# print("Now let's verify that we have enough spans tracked properly...")
# try:
#     # LangGraph doesn't emit LLM spans in the same format, so we just check span count
#     result = agentops.validate_trace_spans(trace_context=None, check_llm=False, min_spans=5)
#     print(f"\n✅ Success! {result['span_count']} spans were properly recorded in AgentOps.")
# except agentops.ValidationError as e:
#     print(f"\n❌ Error validating spans: {e}")
#     raise


import os
os.environ['ARIZE_API_KEY'] = "ak-"

from arize.exporter import ArizeExportClient
from arize.utils.types import Environments
from datetime import datetime, timedelta, timezone
from phoenix.evals import llm_classify, OpenAIModel
import nest_asyncio, os
import pandas as pd

 
client = ArizeExportClient()  

api_key = "dddM"
project_name = "Suresh_test"

 
df = client.export_model_to_df(
    space_id = "U3BhY2U6MjYxODI6cUlwUg==",
    model_id="Suresh_test",  #Project name
    environment = Environments.TRACING,
    start_time = datetime.now(timezone.utc) - timedelta(days=7),
    end_time   = datetime.now(timezone.utc),
)
 
# Example: Aggregate all outputs in a trace
trace_df = (
    df.groupby("context.trace_id")
      .agg({
          "attributes.input.value": "first",
          "attributes.output.value": lambda x: " ".join(x.dropna()),
          # Add other aggregations as needed
      })
)

TRACE_EVAL_PROMPT = """
You are evaluating the overall quality and correctness of an LLM application's response to a user request.

You will be given:
1. The user input that initiated the trace
2. The full output(s) generated during the trace

##
User Input:
{attributes.input.value}

Trace Output:
{attributes.output.value}
##

Respond with exactly one word: `correct` or `incorrect`.
- `correct` → the trace achieves the intended goal.
- `incorrect` → the trace fails to achieve the goal or is low quality.
"""


nest_asyncio.apply()

model = OpenAIModel(
    api_key = os.environ["OPENAI_API_KEY"],
    model   = "gpt-4o-mini",
    temperature = 0.0,
)

rails = ["correct", "incorrect"]
results = llm_classify(
    dataframe           = trace_df,
    template            = TRACE_EVAL_PROMPT,
    model               = model,
    rails               = rails,
    provide_explanation = True,   
    include_prompt = True,
    verbose             = False,
)

results.to_csv("arize_evals.csv")

print(results)


flowchart TD
    A([User Login to Studio])
    B([Go to Control Plane])
    C([Trigger Agent Start])
    D([Agent: Start Process])
    E([Lookup Dependencies<br>from JSON File])
    F([Fetch Running Agents<br>from Agentic Registry])
    G([Configure Fetched Agents])
    H([Fetch Running MCP Servers])
    I([Configure MCP Servers])
    J([Register Agent<br>in Registry])
    K([Agent Ready to<br>Take Traffic])

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> K



import base64
import json
from cryptography.hazmat.primitives.asymmetric import rsa
from cryptography.hazmat.primitives import serialization

def base64url_to_int(val):
    """Convert base64url string to integer."""
    val += '=' * ((4 - len(val) % 4) % 4)  # padding
    decoded = base64.urlsafe_b64decode(val)
    return int.from_bytes(decoded, 'big')

def jwk_to_pem(jwk):
    """Convert RSA JWK to PEM public key."""
    n = base64url_to_int(jwk['n'])
    e = base64url_to_int(jwk['e'])
    public_numbers = rsa.RSAPublicNumbers(e, n)
    public_key = public_numbers.public_key()
    pem = public_key.public_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PublicFormat.SubjectPublicKeyInfo
    )
    return pem

# Example usage:
jwks = {
  # your JWKS fetched from https://issuer/.well-known/jwks.json
}

kid = 'kid-from-jwt-header'
key = None
for k in jwks['keys']:
    if k['kid'] == kid:
        key = k
        break

if key:
    pem_key = jwk_to_pem(key)
    # Now use pem_key (bytes) as the public key in PyJWT decode
    # e.g., jwt.decode(token, pem_key, algorithms=['RS256'], options={...})




import functools
import jwt
from fastmcp import MCPError, request_context

SECRET_KEY = "your-signing-secret"     # Or load from env/KeyVault
ALGORITHM = "HS256"
ISSUER = "https://auth.example.com"

def authenticate(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        auth_header = request_context.headers.get("Authorization", "")
        if not auth_header.startswith("Bearer "):
            raise MCPError(401, "Missing or invalid Authorization header")
        token = auth_header.split(" ", 1)[1]
        try:
            claims = jwt.decode(
                token,
                SECRET_KEY,
                algorithms=[ALGORITHM],
                issuer=ISSUER,
                options={"require": ["exp", "iss", "sub"]}
            )
        except jwt.ExpiredSignatureError:
            raise MCPError(401, "Token has expired")
        except jwt.InvalidTokenError as e:
            raise MCPError(401, f"Invalid token: {str(e)}")
        # Attach claims to context for downstream use
        request_context.user = claims["sub"]
        request_context.entitlements = set(claims.get("entitlements", []))
        return func(*args, **kwargs)
    return wrapper


def require_entitlement(entitlement_name: str):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            user_ent = getattr(request_context, "entitlements", set())
            if entitlement_name not in user_ent:
                raise MCPError(403, f"Missing entitlement: {entitlement_name}")
            return func(*args, **kwargs)
        return wrapper
    return decorator


from fastmcp import tool

@tool("list_tools")
@authenticate
@require_entitlement("tools:list")
def list_tools():
    # business logic to list available tools
    return {"tools": [...]}

@tool("execute_tool")
@authenticate
@require_entitlement("tools:execute")
def execute_tool(tool_name: str, params: dict):
    # business logic for tool execution
    result = run_tool(tool_name, params)
    return {"result": result}


jwt.encode({
    "sub": user_id,
    "iss": ISSUER,
    "exp": expiration_timestamp,
    "entitlements": ["tools:list", "tools:execute", ...]
}, SECRET_KEY, algorithm=ALGORITHM)


